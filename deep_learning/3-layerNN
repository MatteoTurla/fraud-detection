{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "df = df.loc[(df[\"TX_DATETIME\"] >= \"2018-06-10\"), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TRANSACTION_ID</th>\n",
       "      <th>TX_DATETIME</th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>TERMINAL_ID</th>\n",
       "      <th>TX_AMOUNT</th>\n",
       "      <th>TX_FRAUD</th>\n",
       "      <th>n_tx_1</th>\n",
       "      <th>n_tx_7</th>\n",
       "      <th>n_tx_30</th>\n",
       "      <th>...</th>\n",
       "      <th>n_tx_terminal_30</th>\n",
       "      <th>n_fraud_terminal_1</th>\n",
       "      <th>n_fraud_terminal_7</th>\n",
       "      <th>n_fraud_terminal_30</th>\n",
       "      <th>tx_terminal_risk_1</th>\n",
       "      <th>tx_terminal_risk_7</th>\n",
       "      <th>tx_terminal_risk_30</th>\n",
       "      <th>tx_weekend</th>\n",
       "      <th>tx_night</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>383220</th>\n",
       "      <td>383221</td>\n",
       "      <td>671282.0</td>\n",
       "      <td>2018-06-10 00:00:42</td>\n",
       "      <td>1896</td>\n",
       "      <td>4610</td>\n",
       "      <td>25.51</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383221</th>\n",
       "      <td>383222</td>\n",
       "      <td>671283.0</td>\n",
       "      <td>2018-06-10 00:01:51</td>\n",
       "      <td>2047</td>\n",
       "      <td>7977</td>\n",
       "      <td>74.96</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383222</th>\n",
       "      <td>383223</td>\n",
       "      <td>671284.0</td>\n",
       "      <td>2018-06-10 00:01:59</td>\n",
       "      <td>4660</td>\n",
       "      <td>9897</td>\n",
       "      <td>20.74</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383223</th>\n",
       "      <td>383224</td>\n",
       "      <td>671285.0</td>\n",
       "      <td>2018-06-10 00:02:37</td>\n",
       "      <td>1949</td>\n",
       "      <td>1083</td>\n",
       "      <td>41.90</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383224</th>\n",
       "      <td>383225</td>\n",
       "      <td>671286.0</td>\n",
       "      <td>2018-06-10 00:02:43</td>\n",
       "      <td>3149</td>\n",
       "      <td>2892</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  TRANSACTION_ID          TX_DATETIME  CUSTOMER_ID  \\\n",
       "383220      383221        671282.0  2018-06-10 00:00:42         1896   \n",
       "383221      383222        671283.0  2018-06-10 00:01:51         2047   \n",
       "383222      383223        671284.0  2018-06-10 00:01:59         4660   \n",
       "383223      383224        671285.0  2018-06-10 00:02:37         1949   \n",
       "383224      383225        671286.0  2018-06-10 00:02:43         3149   \n",
       "\n",
       "        TERMINAL_ID  TX_AMOUNT  TX_FRAUD  n_tx_1  n_tx_7  n_tx_30  ...  \\\n",
       "383220         4610      25.51         0       1       3       15  ...   \n",
       "383221         7977      74.96         0       4      13       65  ...   \n",
       "383222         9897      20.74         0       6      22       86  ...   \n",
       "383223         1083      41.90         0       3      19       77  ...   \n",
       "383224         2892       3.65         0       3      20       74  ...   \n",
       "\n",
       "        n_tx_terminal_30  n_fraud_terminal_1  n_fraud_terminal_7  \\\n",
       "383220                41                   0                   0   \n",
       "383221                51                   0                   0   \n",
       "383222                38                   0                   0   \n",
       "383223                35                   0                   0   \n",
       "383224                29                   0                   0   \n",
       "\n",
       "        n_fraud_terminal_30  tx_terminal_risk_1  tx_terminal_risk_7  \\\n",
       "383220                    0                 0.0                 0.0   \n",
       "383221                    0                 0.0                 0.0   \n",
       "383222                    0                 0.0                 0.0   \n",
       "383223                    0                 0.0                 0.0   \n",
       "383224                    2                 0.0                 0.0   \n",
       "\n",
       "        tx_terminal_risk_30  tx_weekend  tx_night  target  \n",
       "383220             0.000000           0         1      no  \n",
       "383221             0.000000           0         1      no  \n",
       "383222             0.000000           0         1      no  \n",
       "383223             0.000000           0         1      no  \n",
       "383224             0.068966           0         1      no  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"TX_AMOUNT\", \n",
    "             \"n_tx_1\", \"n_tx_7\", \"n_tx_30\", \n",
    "             \"avg_tx_1\", \"avg_tx_7\" , \"avg_tx_30\" , \n",
    "             \"n_tx_terminal_1\", \"n_tx_terminal_7\" , \"n_tx_terminal_30\" , \n",
    "             \"tx_terminal_risk_1\" , \"tx_terminal_risk_7\" , \"tx_terminal_risk_30\" ,\n",
    "             \"tx_weekend\" , \"tx_night\"]\n",
    "\n",
    "train = df.loc[(df[\"TX_DATETIME\"] >= \"2018-06-10\" )&(df[\"TX_DATETIME\"] < \"2018-06-17\" ), :]\n",
    "test = df.loc[(df[\"TX_DATETIME\"] >= \"2018-06-24\" )&(df[\"TX_DATETIME\"] < \"2018-07-01\" ), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[features]\n",
    "y_train = train[[\"TX_FRAUD\"]].values\n",
    "\n",
    "x_test = test[features]\n",
    "y_test = test[[\"TX_FRAUD\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(sparse=False)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = StandardScaler()\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "ss.fit(x_train)\n",
    "ohe.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = ss.transform(x_train)\n",
    "x_test = ss.transform(x_test)\n",
    "\n",
    "pos = (y_train == 1).sum()\n",
    "neg = (y_train == 0).sum()\n",
    "y_train = ohe.transform(y_train)\n",
    "y_test = ohe.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.5975 - val_loss: 0.5358\n",
      "Epoch 2/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4725 - val_loss: 0.4124\n",
      "Epoch 3/100\n",
      "65/65 [==============================] - 0s 983us/step - loss: 0.3509 - val_loss: 0.3047\n",
      "Epoch 4/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2529 - val_loss: 0.2264\n",
      "Epoch 5/100\n",
      "65/65 [==============================] - 0s 982us/step - loss: 0.1834 - val_loss: 0.1736\n",
      "Epoch 6/100\n",
      "65/65 [==============================] - 0s 941us/step - loss: 0.1368 - val_loss: 0.1385\n",
      "Epoch 7/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1061 - val_loss: 0.1151\n",
      "Epoch 8/100\n",
      "65/65 [==============================] - 0s 949us/step - loss: 0.0858 - val_loss: 0.0988\n",
      "Epoch 9/100\n",
      "65/65 [==============================] - 0s 968us/step - loss: 0.0720 - val_loss: 0.0873\n",
      "Epoch 10/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0624 - val_loss: 0.0788\n",
      "Epoch 11/100\n",
      "65/65 [==============================] - 0s 971us/step - loss: 0.0555 - val_loss: 0.0723\n",
      "Epoch 12/100\n",
      "65/65 [==============================] - 0s 979us/step - loss: 0.0503 - val_loss: 0.0667\n",
      "Epoch 13/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0463 - val_loss: 0.0623\n",
      "Epoch 14/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0432 - val_loss: 0.0582\n",
      "Epoch 15/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0406 - val_loss: 0.0553\n",
      "Epoch 16/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0385 - val_loss: 0.0526\n",
      "Epoch 17/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0368 - val_loss: 0.0508\n",
      "Epoch 18/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0353 - val_loss: 0.0491\n",
      "Epoch 19/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0341 - val_loss: 0.0477\n",
      "Epoch 20/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0331 - val_loss: 0.0465\n",
      "Epoch 21/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0322 - val_loss: 0.0455\n",
      "Epoch 22/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0315 - val_loss: 0.0448\n",
      "Epoch 23/100\n",
      "65/65 [==============================] - 0s 970us/step - loss: 0.0308 - val_loss: 0.0442\n",
      "Epoch 24/100\n",
      "65/65 [==============================] - 0s 957us/step - loss: 0.0302 - val_loss: 0.0436\n",
      "Epoch 25/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0296 - val_loss: 0.0433\n",
      "Epoch 26/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0291 - val_loss: 0.0429\n",
      "Epoch 27/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0287 - val_loss: 0.0426\n",
      "Epoch 28/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0282 - val_loss: 0.0424\n",
      "Epoch 29/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0278 - val_loss: 0.0422\n",
      "Epoch 30/100\n",
      "65/65 [==============================] - 0s 998us/step - loss: 0.0274 - val_loss: 0.0420\n",
      "Epoch 31/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0271 - val_loss: 0.0418\n",
      "Epoch 32/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0268 - val_loss: 0.0416\n",
      "Epoch 33/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0264 - val_loss: 0.0415\n",
      "Epoch 34/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0261 - val_loss: 0.0413\n",
      "Epoch 35/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0258 - val_loss: 0.0412\n",
      "Epoch 36/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0256 - val_loss: 0.0411\n",
      "Epoch 37/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0253 - val_loss: 0.0411\n",
      "Epoch 38/100\n",
      "65/65 [==============================] - 0s 946us/step - loss: 0.0250 - val_loss: 0.0409\n",
      "Epoch 39/100\n",
      "65/65 [==============================] - 0s 995us/step - loss: 0.0248 - val_loss: 0.0409\n",
      "Epoch 40/100\n",
      "65/65 [==============================] - 0s 958us/step - loss: 0.0245 - val_loss: 0.0408\n",
      "Epoch 41/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0243 - val_loss: 0.0406\n",
      "Epoch 42/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0241 - val_loss: 0.0405\n",
      "Epoch 43/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0239 - val_loss: 0.0404\n",
      "Epoch 44/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0236 - val_loss: 0.0403\n",
      "Epoch 45/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0234 - val_loss: 0.0402\n",
      "Epoch 46/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0232 - val_loss: 0.0402\n",
      "Epoch 47/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0231 - val_loss: 0.0400\n",
      "Epoch 48/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0229 - val_loss: 0.0399\n",
      "Epoch 49/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0227 - val_loss: 0.0399\n",
      "Epoch 50/100\n",
      "65/65 [==============================] - 0s 948us/step - loss: 0.0226 - val_loss: 0.0397\n",
      "Epoch 51/100\n",
      "65/65 [==============================] - 0s 982us/step - loss: 0.0224 - val_loss: 0.0397\n",
      "Epoch 52/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0222 - val_loss: 0.0395\n",
      "Epoch 53/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0221 - val_loss: 0.0395\n",
      "Epoch 54/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0220 - val_loss: 0.0394\n",
      "Epoch 55/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0218 - val_loss: 0.0395\n",
      "Epoch 56/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0217 - val_loss: 0.0391\n",
      "Epoch 57/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0216 - val_loss: 0.0393\n",
      "Epoch 58/100\n",
      "65/65 [==============================] - 0s 985us/step - loss: 0.0214 - val_loss: 0.0392\n",
      "Epoch 59/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0213 - val_loss: 0.0390\n",
      "Epoch 60/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0212 - val_loss: 0.0390\n",
      "Epoch 61/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0211 - val_loss: 0.0389\n",
      "Epoch 62/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0210 - val_loss: 0.0388\n",
      "Epoch 63/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0209 - val_loss: 0.0386\n",
      "Epoch 64/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0208 - val_loss: 0.0386\n",
      "Epoch 65/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0207 - val_loss: 0.0385\n",
      "Epoch 66/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0206 - val_loss: 0.0384\n",
      "Epoch 67/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0205 - val_loss: 0.0385\n",
      "Epoch 68/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0204 - val_loss: 0.0381\n",
      "Epoch 69/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0204 - val_loss: 0.0384\n",
      "Epoch 70/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0203 - val_loss: 0.0382\n",
      "Epoch 71/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0202 - val_loss: 0.0381\n",
      "Epoch 72/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0201 - val_loss: 0.0379\n",
      "Epoch 73/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0201 - val_loss: 0.0382\n",
      "Epoch 74/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0200 - val_loss: 0.0382\n",
      "Epoch 75/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0199 - val_loss: 0.0380\n",
      "Epoch 76/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0199 - val_loss: 0.0378\n",
      "Epoch 77/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0198 - val_loss: 0.0379\n",
      "Epoch 78/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0375\n",
      "Epoch 79/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0378\n",
      "Epoch 80/100\n",
      "65/65 [==============================] - 0s 961us/step - loss: 0.0196 - val_loss: 0.0377\n",
      "Epoch 81/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0195 - val_loss: 0.0377\n",
      "Epoch 82/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0195 - val_loss: 0.0375\n",
      "Epoch 83/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0195 - val_loss: 0.0376\n",
      "Epoch 84/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0194 - val_loss: 0.0374\n",
      "Epoch 85/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0193 - val_loss: 0.0375\n",
      "Epoch 86/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0193 - val_loss: 0.0374\n",
      "Epoch 87/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0192 - val_loss: 0.0376\n",
      "Epoch 88/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0192 - val_loss: 0.0375\n",
      "Epoch 89/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0192 - val_loss: 0.0378\n",
      "Epoch 90/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0191 - val_loss: 0.0374\n",
      "Epoch 91/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0191 - val_loss: 0.0373\n",
      "Epoch 92/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0190 - val_loss: 0.0372\n",
      "Epoch 93/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0190 - val_loss: 0.0373\n",
      "Epoch 94/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0190 - val_loss: 0.0373\n",
      "Epoch 95/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0189 - val_loss: 0.0370\n",
      "Epoch 96/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0189 - val_loss: 0.0369\n",
      "Epoch 97/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0188 - val_loss: 0.0371\n",
      "Epoch 98/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0188 - val_loss: 0.0373\n",
      "Epoch 99/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0188 - val_loss: 0.0370\n",
      "Epoch 100/100\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0187 - val_loss: 0.0371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffc94e2b1f0>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(8, activation='relu'),\n",
    "  tf.keras.layers.Dense(8, activation='relu'),\n",
    "  tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "              )\n",
    "\n",
    "model.fit(x_train[:-1000], y_train[:-1000],\n",
    "          epochs=100, \n",
    "          batch_size=1024,\n",
    "          validation_data=(x_train[-1000:-1], y_train[-1000:-1]),      \n",
    "\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test).argmax(1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[66596,   287],\n",
       "       [   24,   341]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test.argmax(axis=1), model.predict(x_test).argmax(axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7229779584521461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6642984211010204"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "print(average_precision_score(y_train.argmax(1), model.predict(x_train)[:,1]))\n",
    "average_precision_score(y_test.argmax(1), model.predict(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9193053583759743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8674445758527785"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(roc_auc_score(y_train.argmax(1), model.predict(x_train)[:,1]))\n",
    "roc_auc_score(y_test.argmax(1), model.predict(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteoturla/opt/anaconda3/envs/ml/lib/python3.8/site-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/Users/matteoturla/opt/anaconda3/envs/ml/lib/python3.8/site-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "train.loc[:, \"predictions\"] = model.predict(x_train)[:, 1]\n",
    "test.loc[:, \"predictions\"] = model.predict(x_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_at_k(df, k):\n",
    "    group = df. \\\n",
    "        sort_values([\"predictions\"], ascending=False). \\\n",
    "        groupby(pd.to_datetime(df[\"TX_DATETIME\"]).dt.date). \\\n",
    "        head(100)\n",
    "    p = group.groupby(pd.to_datetime(df[\"TX_DATETIME\"]).dt.date)[\"TX_FRAUD\"].sum()/100\n",
    "    return p.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5885714285714284"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_at_k(test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5757142857142857"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_at_k(train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
